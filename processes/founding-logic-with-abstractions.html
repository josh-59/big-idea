<!doctype html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>Founding Logic with Abstractions</title>
	<link href="../style.css" rel="stylesheet" type="text/css">
	<script>
	MathJax = {
	  tex: {
	    inlineMath: [['$', '$'], ['\\(', '\\)']]
	  }
	};
	</script>
	<script type="text/javascript" id="MathJax-script" async
	  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
	</script>
</head>

<body>
	<header>
		<nav>
			<ul>
				<li> <a href="../"> Home </a></li>
				<li> <a href="./"> Processes </a></li>
				<li> <a href="../about.html"> About </a></li>
			</ul>
		</nav>


	</header>

	<br>

	<main>
		<h1> Founding Logic with Abstractions </h1>
		<h2> Introduction </h2>

		<p>
		    Many people have written about their interpretation of logic.
            this is mine.  
            I will not attempt to produce a formal definition of logic.
            Rather, I will try to explain <em>what it's about.</em>
        </p>

        <p>
		    Logic is about thinking clearly.
		    As such, it is, in some sense, a formal rendering of clear thought.
		    To that end, there are two characteristic features of the activity.
            The first is being explicit;
            the second is not presupposing connections.
		</p>

        <p>
            What we aim to do, in logic, is reconsider our own thinking.
            By considering our thinking in the abstract (without subject matter),
            we hope to establish the validity of our thought, then carry that
            validity back to the subject of our choosing.
        </p>

        <p>
            The standard logic of our time is, <em>First-Order Logic.</em>
            It is sufficiently general to apply to any science of our day (I believe);
            it is <em>complete</em>; it is well-developed.
            This will be our pursuit.
        </p>

        
        <p>
            Logic is a large subject, and one that I have yet to spend the requisite
            lifetime studying; the history of logic is even larger.
            What I do have are some novel thoughts on the structure and purpose of the subject.
            In addition, I have a wish to develop First-Order Logic in common-sense terms, in part just
            to develop a personal understanding of it.
            So, think of this work as a study guide to logic.
        </p>

        <p>
            There are many books that develop the First-Order Logic. 
            Two good ones are, 
            <em>Language, Proof and Logic</em>, 2<sup>nd</sup> ed., by Dave Barker-Plumer,
			Jon Barwise & John Etchemendy; and
			<em>An Introduction to Logic</em> by Patrick Suppes.
			We will use these frequently as reference.
		</p>

        <h2> Starting Out </h2>
        <p>
            To start, we require a subject to consider. 
            Such a subject we will call a <em>language</em>; if the language abides by a first-order logic, 
            then it is called a <em>first-order lanugage.</em>
            Noting that all math and all science (that I know of) abide by first-order logic, we can
            say that they are each first-order languages.
            The rationale behind these choice names will be developed in due course, but for now note that they
            are in keeping with both <em>Language, Proof and Logic,</em> and <em>An Introduction to Logic</em>.
        </p>

        <p>
            Within <em>Language, Proof and Logic,</em> hereafter referred to by <em>LPL</em>, the authors
            develop three first-order languages.  The first is relatively minor, composed of sentences like,
            "Saw(max, claire)," translated as <em>Max saw Claire</em>, and "Gave(claire, max)", translated
            <em>Claire gave Scruffy to Max</em>.
            The second is a major feature of the work, and is called "Tarski's World."
            It has sentences like, <em>Cube(b)</em>, translated, "<em>b</em> is a cube."
            The third language developed in LPL is set theory.
        </p>

        <p>
            Dr. Patrick Suppes has written two works, which are complimentary.
            <em>An Introduction to Logic</em> develops primarily logic, using set theory as its exemplar language;
            <em>Axiomatic Set Theory</em> develops primarily set theory, using first-order logic to generate its
            rigorous development.
        </p>

		<h2> Objects </h2>

		<p>
			We begin with objects, namely, because they are fundamental to first-order logic.
			In Section 1.1 of LPL, we find:

			<div class="quote_text">
				"Individual constants are simply symbols that are used to refer to some fixed individual object."
			</div>
		</p>

        <p>
            Later (Section 1.4), we find:
            <div class="quote_text">
                "But we construe the notion of an "object" pretty flexibly&mdash; to cover anything that we can make
                about."
            </div>
        </p>

		<p>
			I say that an object is a well-defined abstraction.
			This means that it is a simplification (intuitively, a subset of all observable facets) of something else, with the caveat that
			it is unique:  There is exactly one.
			Objects transcend language;
            they belong to the mind alone.
		</p>

		<p>
			It's worth giving a few examples of objects, before proceeding.
			For instance, a pen is an object, as is a book.
			In particular, we make no distinction between objects that can be "realized by"
			some physical entity and those that cannot.
			Therefore, the color <em>blue</em> is an object just as much as <em>a bottle</em> is an object.
		</p>

		<p>
			This uniformity is justified in large part by my interpretation of mathematics:
			The number <em>two</em> is an object (amongst mathematicians, this is beyond dispute),
			and the number two is not "realized by" any physical entities.
			Rather, it is a well-defined abstraction, which we
			laboriously consider as though it really existed.
		</p>

		<h2> Names </h2>
		<p>
			In order to talk about an object, it is necessary to have a tangible reference
			to it.
			The usual words are <em>symbol</em>, for the "tangible reference" part,
		 	and <em>denote</em>, for the "refers to" part.
			The situation can be likened to pointers in C:
		</p>

		<a href="img\pointer.png" title="Click for larger image"><img src="img/pointer.png"></a>

		<p>
			Intuitively, symbols are names for objects.
			There my be many names for a given object, or there may be none, but first-order logic
            requires that each name refer to exactly one object (LPL, page 20).
		</p>

		<p>
			For example, at the lower levels of mathematics, numerals, such as $1$,
			denote quantities, whereas in higher levels of mathematics,
			this is occasionally called into question.
			This might be done in order to develop the natural numbers in terms of sets,
			for instance.
			Within such a context, $1$ would be considered a symbol, and what the symbol
			denotes would be carefully developed.
		</p>

        <p>
            The formal first-order logic terms for "names," as I understand them:
        </p>

        <ul>
            <li>
                <strong>Individual Constant:</strong> 
                    A name that is constant across the entirity of the language.
                    For instance, the symbol $1$ is an individual constant in calculus.
            </li>
            <li>
                <strong>Referent:</strong>
                    The object that a name denotes.
            </li>
        </ul>

        <h2> Making Statements </h2>

		<p>
			Once given that objects may be named, there are a few things that one can say about objects them.
			The most basic thing that one can say is that two names
			<em>denote the same</em> object; this is done through the much-maligned
			equals-sign, more appropriately thought of as <em>identity:</em>
		</p>

		$$
		a = b
		$$

		<p>
			The above says that $a$ refers to the same object as $b.$
		</p>

		<p>
			To give a concrete example, if we stipulate that the symbol $1$ denotes
			the set containing the empty set, then we arrive at the truism that,
		</p>

		$$
			1 = \{\{\}\}
		$$

		<p>
		    I feel it prudent to reference the work of a more authoritative figure
            about such a blanket statement regarding the equals-sign.
            So, on page 4 of <em>Axiomatic Set Theory</em>, Dr. Patrick Suppes has this to say:

            <div class="quote_text">
                "One [logical] principle used, concerning which there is some disagreement in practice
                among mathematicians, is that the double bar '=' is taken as the sign of identity.
                The formula '$x=y$' may be read '$x$ is the same as $y$',  '$x$ is identical with $y$'
                or '$x$ is equal to $y$'.  The last reading is permissible here only if it
                is understood that equality means sameness of identity (which is what it does mean in almost
                all ordinary mathematical contexts)."
            </div>
        </p>

        <p>
            For more on the subject of identity by Suppes, see Chapter 5 of <em>An Introduction to Logic</em>.
        </p>

        <p>
            A typical use of identity in mathematics is found in Walter Rudin's <em>Principles of Mathematical Analysis</em>. 
            In the definition of an ordered set, we find:
        </p>

        <p> <strong> Definition </strong> Let $S$ be a set.  An <em>order</em> on $S$ is a relation, denoted by &lt;,
        with the following two properties:

        <ol> 
            <li> 
                If $x\in S$ and $y\in S$ then one and only one of the statements
                $$
                x < y, \hspace{2em} x = y, \hspace{2em} y < x 
                $$
                is true.
            </li>
            <li> 
                 If $x, y, z \in S$, if $x < y$ and $ y < z$, then $ x < z$.
            </li>
        </ol>
        </p>

        <p>
            Intuitively, 1) States that, given any two elements of an ordered set, either one is larger than the
            other, or they are the same element;
            2) Stipulates transitivity.
        </p>

        </p>
            Both objects and names are mainstays of first-order languages;
			concurrently, <em>identity</em> is often included in first-order languages.
		</p>

		<h2> Sentential / Truth-Functional Connectives </h2>

		<p>
			For example, the semantics of the implication are taught in most
			texts (including <em>An Introduction to Logic</em>) not to depend upon
			any non-truth relation, so that a sentence like,
			$$
			\text{If the sky is blue, then } 2 + 2 = 4
			$$
			is considered a valid use of the implication connective.
			As proof, the above implication is considered <em>true</em>, by modern logicians.
		</p>

		<p>
			This is done for well-devised reasons:
			<ol>
				<li>Given a declarative sentence, we can always determine its truthity</li>
				<li>By defining implication in terms of truth-values, we can make its meaning explicit</li>
			</ol>

			Indirection:
			https://en.wikipedia.org/wiki/Fundamental_theorem_of_software_engineering
		</p>

		<p>
			Another thing one can say about objects, which the developments of
			logic given above do not touch upon, is that one object may be contained in
			another object.
			Tentatively, we will define this as the <em>is</em> relation.
			We define:
			For any two objects $a$ and $b$, "$a$ <em>is</em> $b$" is true if and only
			if $b$ is a correct abstraction of $a.$
		</p>

		<p>
			This is used in natural language to make statements like, "That bottle is blue."
			The abstraction denoted by "bottle" contains the abstraction "blue,"
			and "blue" is a correct abstraction of the bottle;
			we might say things like, "Hand me that blue thing over there."
		</p>

		<h2> About </h2>
		<p>
			My other project, <a href="https://josh-59.github.io/Learning-Linux/">Learning Linux</a>,
			has been remarkably successful in teaching me about Linux.
			In addition, it's taught me to read works of all kinds, because the struggle in writing well
			about Linux has been in using language to express ideas, where the ideas are well-founded, and the language is variable.
			It seemed that a similar pattern was forming in the study of logic: That my own thoughts on logic
			exist, but are not sufficiently defined to be unambiguous, and therefore,
			progress in learning the subject has stalled.
		</p>

		<p>
			So, I endeavor to recapture the success of the Learning Linux project, in the study of logic.
			The two books I'm reading on logic are, <em>Language, Proof and Logic</em>, 2<sup>nd</sup> ed., by Dave Barker-Plumer,
			Jon Barwise & John Etchemendy; and
			<em>An Introduction to Logic</em> by Patrick Suppes.
			 I mention this because these, together, form the foundation of my studies about logic.
			<em>Language, Proof and Logic</em> is a contemporary work on the subject of logic; moreover,
			because it is the work of three authors, I feel that it represents well the subject as it
			is taught today.
			<em>An Introduction to Logic</em> was written in 1957; although it is comparatively dated,
			Patrick Suppes is a remarkable author.
		</p>


			<center> <a href="#">Top </a> </center>
		</p>

		<footer>
			Questions, comments, criticisms? <a href="mailto:josh.m.timmons@gmail.com" > Leave me feedback!</a>
		</footer>
	</main>

</body>
</html>
