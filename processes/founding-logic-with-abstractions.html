<!doctype html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>Founding Logic with Abstractions</title>
	<link href="../style.css" rel="stylesheet" type="text/css">
	<script>
	MathJax = {
	  tex: {
	    inlineMath: [['$', '$'], ['\\(', '\\)']]
	  }
	};
	</script>
	<script type="text/javascript" id="MathJax-script" async
	  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
	</script>
</head>

<body>
	<header>
		<nav>
			<ul>
				<li> <a href="../"> Home </a></li>
				<li> <a href="./"> Processes </a></li>
				<li> <a href="../about.html"> About </a></li>
			</ul>
		</nav>


	</header>

	<br>

	<main>
		<h1> Founding Logic with Abstractions </h1>



		<h2> Introduction </h2>

		<p>

			Logic is <em>very</em> keen not to presuppose connections.
		</p>

		<h2> Objects </h2>

		<p>
			We begin with objects, namely, because this is where I choose to begin; in addition,
			they are assumed in Section 1.1 of LPL:

			<div class="quote_text">
				"Individual constants are simply symbols that are used to refer to some fixed individual object."
			</div>
		</p>

		<p>
			So what is an object?  I say that an object is a well-defined abstraction.
			This means that it is a simplification (intuitively, a subset of all observable facets) of something else, with the caveat that
			it is unique:  There is exactly one.
			Objects transcend language:  They belong to the mind alone.
		</p>

		<p>
			It's worth giving a few examples of objects, before proceeding.
			For instance, a pen is an object, as is a book.
			In particular, we make no distinction between objects that can be "realized by"
			some physical entity and those that cannot.
			Therefore, the color <em>blue</em> is an object just as much as <em>a bottle</em> is an object.
		</p>

		<p>
			This uniformity is justified in large part by my interpretation of mathematics:
			The number <em>two</em> is an object (amongst mathematicians, this is beyond dispute),
			and the number two is not "realized by" any physical entities.
			Rather, it is a well-defined abstraction, which we
			laboriously consider as though it really existed.
		</p>

		<h2> Names </h2>
		<p>
			As said before, logic is <em>very</em> keen not to presuppose connections.
			There are a few good examples of this, but the one I'm concerned with at the moment
			is <em>names</em>.
		</p>

		<p>
			In order to talk about an object, it is necessary to have a tangible reference
			to it.
			The usual words are <em>symbol</em>, for the "tangible reference" part,
		 	and <em>denote</em>, for the "refers to" part.
			The situation can be likened to pointers in C:
		</p>

		<a href="img\pointer.png" title="Click for larger image"><img src="img/pointer.png"></a>

		<p>
			Intuitively, symbols are names for objects.
			There my be many names for a given object, or there may be none.
		</p>


		<p>
			For example, at the lower levels of mathematics, numerals (such as $1$)
			are assumed to denote quantities, whereas at higher levels of mathematics,
			this is occasionally called into question.
			This might be done in order to develop the natural numbers in terms of sets,
			for instance.
			Within such a context, $1$ would be considered a symbol, and what the symbol
			denotes would be carefully developed.
		</p>

		<p>
			Once given that objects may be named, there are a few things that one can say about objects them.
			The most basic thing that one can say is that two names
			<em>denote the same</em> object; this is done through the much-maligned
			equals sign, more appropriately thought of as <em>identity</em>:
		</p>

		$$
		a = b
		$$

		<p>
			The above says that $a$ refers to the same object as $b.$
		</p>

		<p>
			To give a concrete example, if we stipulate that the symbol $1$ denotes
			the set containing the empty set, then we arrive at the truism that,
		</p>

		$$
			1 = \{\{\}\}
		$$

		<p>
			Both objects and names are mainstays of first-order languages;
			concurrently, <em>identity</em> is often included in first-order languages.
		</p>



		<h2> Sentential / Truth-Functional Connectives </h2>

		<p>
			For example, the semantics of the implication are taught in most
			texts (including <em>An Introduction to Logic</em>) not to depend upon
			any non-truth relation, so that a sentence like,
			$$
			\text{If the sky is blue, then } 2 + 2 = 4
			$$
			is considered a valid use of the implication connective.
			As proof, the above implication is considered <em>true</em>, by modern logicians.
		</p>

		<p>
			This is done for well-devised reasons:
			<ol>
				<li>Given a declarative sentence, we can always determine its truthity</li>
				<li>By defining implication in terms of truth-values, we can make its meaning explicit</li>
			</ol>

			Indirection:
			https://en.wikipedia.org/wiki/Fundamental_theorem_of_software_engineering
		</p>

		<p>
			Another thing one can say about objects, which the developments of
			logic given above do not touch upon, is that one object may be contained in
			another object.
			Tentatively, we will define this as the <em>is</em> relation.
			We define:
			For any two objects $a$ and $b$, "$a$ <em>is</em> $b$" is true if and only
			if $b$ is a correct abstraction of $a.$
		</p>

		<p>
			This is used in natural language to make statements like, "That bottle is blue."
			The abstraction denoted by "bottle" contains the abstraction "blue,"
			and "blue" is a correct abstraction of the bottle;
			we might say things like, "Hand me that blue thing over there."
		</p>

		<h2> About </h2>
		<p>
			My other project, <a href="https://josh-59.github.io/Learning-Linux/">Learning Linux</a>,
			has been remarkably successful in teaching me about Linux.
			In addition, it's taught me to read works of all kinds, because the struggle in writing well
			about Linux has been in using language to express ideas, where the ideas are well-founded, and the language is variable.
			It seemed that a similar pattern was forming in the study of logic: That my own thoughts on logic
			exist, but are not sufficiently defined to be unambiguous, and therefore,
			progress in learning the subject has stalled.
		</p>

		<p>
			So, I endeavor to recapture the success of the Learning Linux project, in the study of logic.
			The two books I'm reading on logic are, <em>Language, Proof and Logic</em>, 2<sup>nd</sup> ed., by Barker-Plumer et al., and
			<em>An Introduction to Logic</em> by Patrick Suppes; I mention this because these, together, form the
			foundation of my assumptions about logic.
			<em>Language, Proof and Logic</em> is a contemporary work on the subject of logic; moreover,
			because it is the work of three authors, I feel that it represents well the subject as it
			is taught today.
			<em>An Introduction to Logic</em> was written in 1957; although it is comparatively dated,
			Patrick Suppes is a remarkable author, to such an extent that the many more modern renditions of
			the same material strike me as secondary sources, by comparison.
		</p>


			<center> <a href="#">Top </a> </center>
		</p>

		<footer>
			Questions, comments, criticisms? <a href="mailto:josh.m.timmons@gmail.com" > Leave me feedback!</a>
		</footer>
	</main>

</body>
</html>
